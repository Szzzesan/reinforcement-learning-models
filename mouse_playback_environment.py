import numpy as np
from environment import BaseEnvironment

class MousePlaybackEnvironment(BaseEnvironment):
    """
    An RL environment that replays a pre-processed sequence of transitions
    derived from actual mouse behavioral data.

    This environment ignores the 'action' input in env_step, as the
    next state and reward are predetermined by the data sequence.
    """
    def __init__(self):
        super().__init__()
        self.transitions = None
        self.current_step_index = 0

    def env_init(self, env_info={}):
        """
        Initialize the environment with the pre-processed transition data.

        Args:
            env_info (dict): Must contain the key 'transitions', which is a list
                             of tuples: [(obs_t, action_t, reward_t+1, next_obs_t+1, terminal_flag), ...]
                             as generated by data_loader.py.
        """
        self.transitions = env_info.get('transitions')
        if self.transitions is None or not isinstance(self.transitions, list) or len(self.transitions) == 0:
            raise ValueError("MousePlaybackEnvironment requires a non-empty list of 'transitions' in env_info.")

        # Optional: Store dt if needed for any internal logic, though not strictly necessary for playback
        self.dt = env_info.get("time_step_duration", 0.1)
        print(f"Playback environment initialized with {len(self.transitions)} transitions.")

    def env_start(self):
        """
        Returns the first observation from the pre-processed data.
        """
        self.current_step_index = 0
        if not self.transitions:
             raise RuntimeError("env_start called before env_init or with empty transitions.")

        # The first observation is the 'obs_t' of the first transition tuple
        first_observation = self.transitions[0][0]
        # print(f"Playback env_start: Returning first obs: {np.round(first_observation, 2)}") # Debug print
        return first_observation

    def env_step(self, action):
        """
        Returns the reward, next observation, and terminal flag for the current step
        from the pre-processed data, ignoring the input action.

        Args:
            action: The action taken by the agent (ignored in this environment).

        Returns:
            (float, state, Boolean): reward, next_observation, terminal_flag
                                     for the current step index.
        """
        if self.transitions is None:
            raise RuntimeError("env_step called before env_init.")
        if self.current_step_index >= len(self.transitions):
            raise IndexError("Playback environment stepped beyond the end of the transitions list.")

        # Retrieve the outcome of the current step from the pre-loaded data
        current_transition = self.transitions[self.current_step_index]
        _obs_t, _action_t, reward_t_plus_1, next_obs_t_plus_1, terminal_flag = current_transition


        # --- Move to the next step in the data sequence ---
        self.current_step_index += 1

        # print(f"Playback env_step {self.current_step_index}: R={reward_t_plus_1}, NextObs={np.round(next_obs_t_plus_1, 2)}, Term={terminal_flag}") # Debug print
        return (reward_t_plus_1, next_obs_t_plus_1, terminal_flag)

    def env_cleanup(self):
        """Cleanup done after the environment ends"""
        self.transitions = None
        self.current_step_index = 0

    def env_message(self, message):
        """A message asking the environment for information"""
        if message == "get_num_steps_total":
             return len(self.transitions) if self.transitions else 0
        return None